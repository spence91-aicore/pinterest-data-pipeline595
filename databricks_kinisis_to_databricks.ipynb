{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Read Data from Kenesis into Databricks\n",
    "\n",
    "##  Retriving the authentication credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "import urllib\n",
    "\n",
    "table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\" \n",
    "aws_keys_df = spark.read.format(\"delta\").load(table_path) \n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable format checking\n",
    "\n",
    "Disable format checks during the reading of Delta tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the connections to the three Kinesis streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# pintrest\n",
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-124a514b9149-pin') \\\n",
    ".option('initialPosition','trim_horizon') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "schema_pin = StructType([StructField(\"index\", StringType(), True), \\\n",
    " StructField(\"unique_id\", StringType(), True), \\\n",
    " StructField(\"title\", StringType(), True), \\\n",
    " StructField(\"description\", StringType(), True), \\\n",
    " StructField(\"poster_name\", StringType(), True), \\\n",
    " StructField(\"follower_count\", StringType(), True), \\\n",
    " StructField(\"tag_list\", StringType(), True), \\\n",
    " StructField(\"is_image_or_video\", StringType(), True), \\\n",
    " StructField(\"image_src\", StringType(), True), \\\n",
    " StructField(\"save_location\", StringType(), True), \\\n",
    " StructField(\"category\", StringType(), True)])\n",
    "df_pin = df_pin.select(from_json(\"data\", schema_pin).alias(\"jsonData\"))\n",
    "df_pin = df_pin.select(\"jsonData.*\")\n",
    "\n",
    "#geo\n",
    "df_geo = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-124a514b9149-geo') \\\n",
    ".option('initialPosition','trim_horizon') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "schema_geo = StructType([StructField(\"ind\", StringType(), True),\\\n",
    "     StructField(\"timestamp\", StringType(), True), \\\n",
    "     StructField(\"latitude\", StringType(), True), \\\n",
    "     StructField(\"longitude\", StringType(), True), \\\n",
    "     StructField(\"country\", StringType(), True)])\n",
    "df_geo = df_geo.select(from_json(\"data\", schema_geo).alias(\"jsonData\"))\n",
    "df_geo = df_geo.select(\"jsonData.*\")\n",
    "\n",
    "# users\n",
    "df_user = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-124a514b9149-user') \\\n",
    ".option('initialPosition','trim_horizon') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "schema_user = StructType([StructField(\"ind\", StringType(), True), \\\n",
    "     StructField(\"first_name\", StringType(), True),\n",
    "     StructField(\"last_name\", StringType(), True),\n",
    "     StructField(\"age\", StringType(), True),\n",
    "     StructField(\"date_joined\", StringType(), True)])\n",
    "df_user = df_user.select(from_json(\"data\", schema_user).alias(\"jsonData\"))\n",
    "df_user = df_user.select(\"jsonData.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 2 - cleanse the three data streams in Databricks.\n",
    "\n",
    "## cleansing the Pinterest data frame.\n",
    "\n",
    "* Replace empty entries and entries with no relevant data in each column with `Nones`\n",
    "* Perform the necessary transformations on the `follower_count` to ensure every entry is a number. Make sure the data type of this column is an `int`.\n",
    "* Ensure that each column containing numeric data has a numeric data type\n",
    "* Clean the data in the `save_location` column to include only the save location path\n",
    "* Rename the `index` column to `ind`.\n",
    "* Reorder the DataFrame columns to have the following column order:\n",
    "    * `ind`\n",
    "    * `unique_id`\n",
    "    * `title`\n",
    "    * `description`\n",
    "    * `follower_count`\n",
    "    * `poster_name`\n",
    "    * `tag_list`\n",
    "    * `is_image_or_video`\n",
    "    * `image_src`\n",
    "    * `save_location`\n",
    "    * `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, regexp_extract\n",
    "\n",
    "#remove rows where the \"unique_id\" column does not exist\n",
    "df_pin = df_pin.filter(col(\"unique_id\").isNotNull())\n",
    "\n",
    "# Rename the `index` column to `ind`.\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "\n",
    "# Ensure that each column containing numeric data has a numeric data type\n",
    "\n",
    "## convert follower_count to int\n",
    "df_pin = df_pin.withColumn(\n",
    "    \"follower_count\",\n",
    "    when(\n",
    "        col(\"follower_count\").contains('k'),\n",
    "        (regexp_extract(\"follower_count\", r'(\\d+)', 1).cast(\"int\") * 1000)\n",
    "    ).when(\n",
    "        col(\"follower_count\").contains('M'),\n",
    "        (regexp_extract(\"follower_count\", r'(\\d+)', 1).cast(\"int\") * 1000000)\n",
    "    ).otherwise(col(\"follower_count\").cast(\"int\"))\n",
    ")\n",
    "##convert ind to int\n",
    "df_pin = df_pin.withColumn(\"ind\", df_pin[\"ind\"].cast(\"int\"))\n",
    "\n",
    "# Clean the data in the `save_location` column to include only the save location path\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_extract(\"save_location\", r\"/.*\", 0))\n",
    "\n",
    "#re-order columns\n",
    "df_pin = df_pin[[ 'ind', 'unique_id', 'title', 'description', 'follower_count', 'poster_name', 'tag_list', 'is_image_or_video', 'image_src', 'save_location', 'category']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the `df_geo` DataFrame with the following transformations:\n",
    "\n",
    "  \n",
    "* Create a new column `coordinates` that contains an array based on the `latitude` and `longitude` columns\n",
    "* Drop the `latitude` and `longitude` columns from the DataFrame\n",
    "* Convert the `timestamp` column from a string to a timestamp data type\n",
    "* Reorder the DataFrame columns to have the following column order:\n",
    "    * `ind`\n",
    "    * `country`\n",
    "    * `coordinates`\n",
    "    * `timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col, to_timestamp\n",
    "\n",
    "# convert ind to int, and convert lattitude and longitude to float\n",
    "df_geo = df_geo.withColumn('ind', df_geo[\"ind\"].cast('int'))\n",
    "df_geo = df_geo.withColumn('latitude', df_geo[\"latitude\"].cast('float'))\n",
    "df_geo = df_geo.withColumn('longitude', df_geo[\"longitude\"].cast('float'))\n",
    "\n",
    "# Create a new column `coordinates` that contains an array based on the `latitude` and `longitude` columns\n",
    "if 'latitude' in df_geo.columns and 'longitude' in df_geo.columns:\n",
    "    df_geo = df_geo.withColumn('coordinates', array(col('latitude'), col('longitude')))\n",
    "\n",
    "# Drop the `latitude` and `longitude` columns from the DataFrame\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "\n",
    "# Convert the `timestamp` column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "\n",
    "#re-order columns\n",
    "df_geo = df_geo[['ind', 'country', 'coordinates', 'timestamp']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Clean the DataFrame that contains information about users.\n",
    "\n",
    "* Create a new column `user_name` that concatenates the information found in the `first_name` and `last_name` columns\n",
    "* Drop the `first_name` and `last_name` columns from the DataFrame\n",
    "* Convert the `date_joined` column from a string to a timestamp data type\n",
    "* Reorder the DataFrame columns to have the following column order:\n",
    "    * `ind`\n",
    "    * `user_name`\n",
    "    * `age`\n",
    "    * `date_joined`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "# convert ind and age to int\n",
    "df_user = df_user.withColumn('ind', col('ind').cast('int'))\n",
    "df_user = df_user.withColumn('age', col('age').cast('int'))\n",
    "\n",
    "# Create a new column `user_name` that concatenates the information found in the `first_name` and `last_name` columns\n",
    "if 'first_name' in df_user.columns and 'last_name' in df_user.columns:\n",
    "    df_user = df_user.withColumn('user_name', concat(col('first_name'),lit(' '), col('last_name')))\n",
    "\n",
    "# Drop the `first_name` and `last_name` columns from the DataFrame\n",
    "df_user = df_user.drop('first_name', 'last_name')\n",
    "\n",
    "# Convert the `date_joined` column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn('date_joined', to_timestamp(col('date_joined')))\n",
    "\n",
    "# re-order columns\n",
    "df_user = df_user[['ind', 'user_name', 'age', 'date_joined']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - write the cleansed data to tables in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cleansed data to tables \n",
    "\n",
    "df_pin.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/pin_checkpoints/\") \\\n",
    "  .table(\"124a514b9149_pin_table\")\n",
    "\n",
    "df_geo.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/geo_checkpoints/\") \\\n",
    "  .table(\"124a514b9149_geo_table\")\n",
    "\n",
    "df_user.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/user_checkpoints/\") \\\n",
    "  .table(\"124a514b9149_user_table\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
